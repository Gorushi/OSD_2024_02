import sklearn
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import r2_score


df = pd.read_csv('winequality-red.csv')


df.info()


df.isnull().T.any().any()


df.columns = df.columns.str.replace(' ','_')
df.describe()


df.hist(figsize=(10,8))
plt.suptitle('Histograms of All Columns', fontsize=16) # Add title
plt.tight_layout(pad=2.0)  # Adjust layout
plt.show()


pd.set_option('display.max_columns',None) # 전체 열 출력
pd.set_option('display.max_rows',None) # 전체 행 출력
col_names = df.columns
for i in range(0,12):
    print(df[col_names[i]].value_counts().sort_index())
    print('\n')


for i in range(0,11):
    print(col_names[i])
    print(df.groupby('quality')[col_names[i]].describe())
    print('\n')


df.groupby('quality').agg(['var'])


import numpy as np

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Create the heatmap
plt.figure(figsize=(10, 9))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, square=True)

# Customize the plot
plt.title('Heatmap of Correlation Matrix', fontsize=16)
plt.tight_layout()
plt.show()


correlation_matrix


threshold = 0.4  # Define the threshold for correlation

# Get the pairs of columns with correlation above the threshold
high_corr = correlation_matrix.where((abs(correlation_matrix) > threshold) & (correlation_matrix < 1))  # Ignore self-correlation
high_corr_pairs = high_corr.stack()  # Stack to get pairs

# Display the results
print("Pairs of columns with high correlation :", threshold)
print(high_corr_pairs)


# Stack the correlation matrix and filter out NaNs
high_corr_pairs = high_corr.stack().reset_index()

# Create a string representation of each pair
high_corr_pairs['pair'] = high_corr_pairs['level_0'] + ',' + high_corr_pairs['level_1']

# Sort the columns lexicographically (to handle reversed pairs)
high_corr_pairs['pair_sorted'] = high_corr_pairs.apply(lambda x: ','.join(sorted([x['level_0'], x['level_1']])), axis=1)

# Drop duplicates based on the sorted pair
unique_pairs = high_corr_pairs.drop_duplicates(subset='pair_sorted')

# Extract the string column
pairs_result = unique_pairs['pair'].tolist()

for pair in pairs_result:
    print(pair)


# pairs_result.remove('alcohol,quality')
for pair in pairs_result:
    col_list = pair.split(',')
    col_x = col_list[0]
    col_y = col_list[1]

    # Scatter plot
    plt.figure(figsize=(8, 6))  # Set figure size
    sns.regplot(data=df, x=f'{col_x}', y=f'{col_y}')
    
    # Add labels and title
    plt.xlabel(f'{col_x}')
    plt.ylabel(f'{col_y}')
    plt.title(f'Reg Plot between {col_x} and {col_y}', fontsize=16)
    
    # Show the plot
    plt.show()


sns.pairplot(df)


X = np.array(df.iloc[:, :-1]) # quality 제외 모든 column 
y = df['quality']

# 교차 검증 - StratifiedKFold
str_kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)

for train_index, test_index in str_kf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    linear_regression_model = LinearRegression()  
    linear_regression_model.fit(X_train, y_train) 
    
    y_test_predict = linear_regression_model.predict(X_test)
    
    # MSE (평균 제곱 오차) 를 통한 테스트 데이터에서의 모델 성능 판단
    mse = mean_squared_error(y_test, y_test_predict)

    # R-squared (결정 계수) 를 통한 테스트 데이터에서의 모델 성능 판단
    r2 = r2_score(y_test, y_test_predict)
    
    print(f'MSE : {mse}')
    print(f'R-squared : {r2:.2f}')


X = np.array(df.iloc[:, :-1]) # quality 제외 모든 column 
y = df['quality']

# 교차 검증 - KFold
kf = KFold(n_splits = 5, shuffle = True, random_state = 10)

for train_index, test_index in kf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    linear_regression_model = LinearRegression()  
    linear_regression_model.fit(X_train, y_train) 
    
    y_test_predict = linear_regression_model.predict(X_test)
    
    # MSE (평균 제곱 오차) 를 통한 테스트 데이터에서의 모델 성능 판단
    mse = mean_squared_error(y_test, y_test_predict)

    # R-squared (결정 계수) 를 통한 테스트 데이터에서의 모델 성능 판단
    r2 = r2_score(y_test, y_test_predict)
    
    print(f'MSE : {mse}')
    print(f'R-squared : {r2:.2f}')


from sklearn.linear_model import LassoCV

X = np.array(df.iloc[:, :-1]) # quality 제외 모든 column 
y = df['quality']

# 교차 검증 - StratifiedKFold
str_kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)

for train_index, test_index in str_kf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # Automatically find the best alpha
    lasso_cv = LassoCV(alphas=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0], cv=5, random_state=42)
    lasso_cv.fit(X_train, y_train)

    # Best alpha
    print(f"Best Alpha: {lasso_cv.alpha_}")
    
    # Predict on the train set
    y_train_predict = lasso_cv.predict(X_train)

    # Predict on the test set
    y_test_predict = lasso_cv.predict(X_test)
    
    # RMSE (평균 제곱근 오차) 를 통한 모델 성능 판단
    train_rmse = (mean_squared_error(y_train, y_train_predict)) ** 0.5
    test_rmse = (mean_squared_error(y_test, y_test_predict)) ** 0.5

    # R-squared (결정 계수) 를 통한 테스트 데이터에서의 모델 성능 판단
    r2 = r2_score(y_test, y_test_predict)
    
    print(f'Train RMSE : {train_rmse}')
    print(f'Test RMSE : {test_rmse}')
    print(f'R-squared : {r2:.2f}')

    coefficients = pd.DataFrame({
        'Feature': col_names[:-1],
        'Coefficient': lasso_cv.coef_
    }).sort_values(by='Coefficient', ascending=False)
     
    # Visualize feature importance
    plt.figure(figsize=(10, 8))
    sns.barplot(x='Coefficient', y='Feature', data=coefficients, palette='viridis')
    plt.title('Feature Coefficient in Lasso regression', fontsize=16)
    plt.xlabel('Coefficiency', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.show()



